{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bd1d65",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Cluster points in space using K-means and DBSCAN\n",
    "\n",
    "This week's lesson is a simplied version of:  \n",
    "- [Chapter 8 in the Geographic Data Science textbook](https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html)\n",
    "- [Chapter 10 in Geographic Data Science textbook](https://geographicdata.science/book/notebooks/10_clustering_and_regionalization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pysal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(context='paper')\n",
    "\n",
    "from pysal.explore import esda\n",
    "from pysal.lib import weights\n",
    "from numpy.random import seed\n",
    "\n",
    "from esda.moran import Moran\n",
    "from libpysal.weights import Queen, KNN\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12c680",
   "metadata": {},
   "source": [
    "# K-Means: San Diego Geodemographic clustering\n",
    "\n",
    "We return to the San Diego tracts dataset we have used earlier in the book. In this case, we will not only rely on its polygon geometries, but also on its attribute information. The data comes from the American Community Survey\n",
    "(ACS) from 2017. Let us begin by reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcab708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "db = gpd.read_file(\"https://www.dropbox.com/s/g8ete3zligcozzq/sandiego_tracts.gpkg?dl=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e16d5f",
   "metadata": {},
   "source": [
    "To make things easier later on, let us collect the variables we will use to\n",
    "characterize Census tracts. These variables capture different aspects of the \n",
    "socioeconomic reality of each area and, taken together, provide a comprehensive\n",
    "characterization of San Diego as a whole. We thus create a list with the names of the columns we will use later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_variables = [\n",
    "    \"median_house_value\",  # Median house value\n",
    "    \"pct_white\",  # % tract population that is white\n",
    "    \"pct_rented\",  # % households that are rented\n",
    "    \"pct_hh_female\",  # % female-led households\n",
    "    \"pct_bachelor\",  # % tract population with a Bachelors degree\n",
    "    \"median_no_rooms\",  # Median n. of rooms in the tract's households\n",
    "    \"income_gini\",  # Gini index measuring tract wealth inequality\n",
    "    \"median_age\",  # Median age of tract population\n",
    "    \"tt_work\",  # Travel time to work\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37ccde",
   "metadata": {},
   "source": [
    "Let's start building up our understanding of this\n",
    "dataset through both visual and statistical summaries.\n",
    "The first stop is considering the spatial distribution of each variable alone.\n",
    "This will help us draw a picture of the multi-faceted view of the tracts we\n",
    "want to capture with our clustering. Let's use (quantile) choropleth maps for\n",
    "each attribute and compare them side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18954ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
    "# Make the axes accessible with single indexing\n",
    "axs = axs.flatten()\n",
    "# Start a loop over all the variables of interest\n",
    "for i, col in enumerate(cluster_variables):\n",
    "    # select the axis where the map will go\n",
    "    ax = axs[i]\n",
    "    # Plot the map\n",
    "    db.plot(\n",
    "        column=col,\n",
    "        ax=ax,\n",
    "        scheme=\"Quantiles\",\n",
    "        linewidth=0,\n",
    "        cmap=\"RdPu\",\n",
    "    )\n",
    "    # Remove axis clutter\n",
    "    ax.set_axis_off()\n",
    "    # Set the axis title to the name of variable being plotted\n",
    "    ax.set_title(col)\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22338f23",
   "metadata": {},
   "source": [
    "Several visual patterns jump out from the maps, revealing both commonalities as\n",
    "well as differences across the spatial distributions of the individual variables.\n",
    "Several variables tend to increase in value from the east to the west\n",
    "(`pct_rented`, `median_house_value`, `median_no_rooms`, and `tt_work`) while others\n",
    "have a spatial trend in the opposite direction (`pct_white`, `pct_hh_female`,\n",
    "`pct_bachelor`, `median_age`). This will help show the strengths of clustering;\n",
    "when variables have\n",
    "different spatial distributions, each variable contributes distinct \n",
    "information to the profiles of each cluster. However, if all variables display very similar \n",
    "spatial patterns, the amount of useful information across the maps is \n",
    "actually smaller than it appears, so cluster profiles may be much less useful as well.\n",
    "It is also important to consider whether the variables display any\n",
    "spatial autocorrelation, as this will affect the spatial structure of the\n",
    "resulting clusters. \n",
    "\n",
    "Recall from [Chapter 6](06_spatial_autocorrelation) that Moran's I is a commonly used\n",
    "measure for global spatial autocorrelation. We can use it to formalise some of the\n",
    "intuitions built from the maps. Recall from earlier in the book that we will need\n",
    "to represent the spatial configuration of the data points through a spatial weights\n",
    "matrix. We will start with queen contiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0370269",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Queen.from_dataframe(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb8e5f",
   "metadata": {},
   "source": [
    "Now let's calculate Moran's I for the variables being used. This will measure\n",
    "the extent to which each variable contains spatial structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "# Calculate Moran's I for each variable\n",
    "mi_results = [\n",
    "    Moran(db[variable], w) for variable in cluster_variables\n",
    "]\n",
    "# Structure results as a list of tuples\n",
    "mi_results = [\n",
    "    (variable, res.I, res.p_sim)\n",
    "    for variable, res in zip(cluster_variables, mi_results)\n",
    "]\n",
    "# Display on table\n",
    "table = pd.DataFrame(\n",
    "    mi_results, columns=[\"Variable\", \"Moran's I\", \"P-value\"]\n",
    ").set_index(\"Variable\")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6efac2",
   "metadata": {},
   "source": [
    "Each of the variables displays significant positive spatial autocorrelation,\n",
    "suggesting clear spatial structure in the socioeconomic geography of San\n",
    "Diego. This means it is likely the clusters we find will have\n",
    "a non random spatial distribution.\n",
    "\n",
    "Spatial autocorrelation only describes relationships between observations for a\n",
    "single attribute at a time.\n",
    "So, the fact that all of the clustering variables are positively autocorrelated does not\n",
    "say much about how attributes co-vary over space. To explore cross-attribute relationships,\n",
    "we need to consider the spatial correlation between variables. We will take our first dip\n",
    "in this direction exploring the bivariate correlation in the maps of covariates themselves.\n",
    "This would mean that we would be comparing each pair of choropleths to look for associations\n",
    "and differences. Given there are nine attributes, there are 36 pairs of maps that must be\n",
    "compared. \n",
    "\n",
    "This would be too many maps to process visually. Instead, we focus directly\n",
    "on the bivariate relationships between each pair of attributes, devoid for now of geography, and use a scatterplot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fa222",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(\n",
    "    db[cluster_variables], kind=\"reg\", diag_kind=\"kde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2f8e9",
   "metadata": {},
   "source": [
    "Two different types of plots are contained in the scatterplot matrix. On the\n",
    "diagonal are the density functions for the nine attributes. These allow for an\n",
    "inspection of the univariate distribution of the values for each attribute.\n",
    "Examining these we see that our selection of variables includes some that are\n",
    "negatively skewed (`pct_white` and `pct_hh_female`) as well as positively skewed\n",
    "(`median_house_value`, `pct_bachelor`, and `tt_work`).\n",
    "\n",
    "The second type of visualization lies in the off-diagonal cells of the matrix; \n",
    "these are bi-variate scatterplots. Each cell shows the association between one\n",
    "pair of variables. **Several of these cells indicate positive linear\n",
    "associations (`median_age` Vs. `median_house_value`, `median_house_value` Vs. `median_no_rooms`)\n",
    "while other cells display negative correlation (`median_house_value` Vs. `pct_rented`,\n",
    "`median_no_rooms` Vs. `pct_rented`, and `median_age` Vs. `pct_rented`)**. The one variable\n",
    "that tends to have consistently weak association with the other variables is\n",
    "`tt_work`, and in part this appears to reflect its rather concentrated \n",
    "distribution as seen on the lower right diagonal corner cell.\n",
    "\n",
    "Indeed, this kind of concentration in values is something you need to be very aware of in clustering contexts. *Distances between datapoints* are of paramount importance in clustering applications. In fact, (dis)similarity between observations is calculated as the statistical distance between themselves. **Because distances are sensitive to the units of measurement, cluster solutions can change when you re-scale your data.**\n",
    "\n",
    "For example, say we locate an observation based on only two variables: house price and gini coefficient. In this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[[\"income_gini\", \"median_house_value\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26a1c2",
   "metadata": {},
   "source": [
    "The distance between observations in terms of these variates can be computed easily using `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ff365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.pairwise_distances(\n",
    "    db[[\"income_gini\", \"median_house_value\"]].head()\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889de66",
   "metadata": {},
   "source": [
    "In this case, we know that the housing values are in the hundreds of thousands, but the Gini coefficient (which we discussed in the previous chapter) is constrained to fall between zero and one. So, for example, the distance between the first two observations is nearly totally driven by the difference in median house value (which is 259100 dollars) and ignores the difference in the Gini coefficient (which is about .11). Indeed, a change of a single dollar in median house value will correspond to *the maximum possible* difference in Gini coefficients. So, a clustering algorithm that uses this distance to determine classifications will pay a lot of attention to median house value, but very little to the Gini coefficient! \n",
    "\n",
    "Therefore, *as a rule*, we standardize our data when clustering. There are many different methods of standardization offered in the `sklearn.preprocessing` module, and these map onto the main methods common in applied work. We review a small subset of them here. The `scale()` method subtracts the mean and divides by the standard deviation:\n",
    "\n",
    "$$ z = \\frac{x_i - \\bar{x}}{\\sigma_x}$$\n",
    "\n",
    "This \"normalizes\" the variate, ensuring the re-scaled variable has a mean of zero and a variance of one. However, the variable can still be quite skewed, bimodal, etc, and insofar as the mean and vairance may be affected by outliers in a given variate, the scaling can be too dramatic. One alternative intended to handle outliers better is `robust_scale()`, which uses the median and the interquartile range in the same fashion:\n",
    "\n",
    "$$ z = \\frac{x_i - \\tilde{x}}{\\lceil x \\rceil_{75} - \\lceil x \\rceil_{25}}$$\n",
    "\n",
    "where $\\lceil x \\rceil_p$ represents the value of the $p$th percentile of $x$. Alternatively, sometimes it is useful to ensure that the maximum of a variate is $1$ and the minimum is zero. In this instance, the `minmax_scale()` is appropriate: \n",
    "\n",
    "$$ z = \\frac{x - min(x)}{max(x-min(x))} $$\n",
    "\n",
    "In most clustering problems, the `robust_scale()` or `scale()` methods are useful. Further, transformations of the variate (such as log-transforming or Box-Cox transforms) can be used to nonlinearly rescale the variates, but these generally should be done before the above kinds of scaling. Here, we will analyze robust-scaled variables. To detach the scaling from the analysis, we will perform the former now, creating a scaled view of our data which we can use later for clustering. For this, we import the scaling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda19433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "## robust_scale is a function that scales the data to have a median of 0 and a\n",
    "## standard deviation of 1. This is useful for clustering algorithms that are\n",
    "## sensitive to the scale of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2ea3b",
   "metadata": {},
   "source": [
    "And create the `db_scaled` object which contains only the variables we are interested in, scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[cluster_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scaled = robust_scale(db[cluster_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383432e5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In conclusion, exploring the univariate and bivariate relationships is a good first step into building\n",
    "a fully multivariate understanding of a dataset. To take it to the next level, we would\n",
    "want to know to what extent these pair-wise relationships hold across different attributes,\n",
    "and whether there are patterns in the \"location\" of observations within the scatter plots.\n",
    "For example, do nearby dots in each scatterplot of the matrix represent the _same_ observations?\n",
    "This type of questions are exactly what clustering helps us explore.\n",
    "\n",
    "## Geodemographic Clusters in San Diego Census Tracts\n",
    "\n",
    "Geodemographic analysis is a form of multivariate\n",
    "clustering where the observations represent geographical areas {cite}`webber2018predictive`. The output\n",
    "of these clusterings is nearly always mapped. Altogether, these methods use\n",
    "multivariate clustering algorithms to construct a known number of\n",
    "clusters ($k$), where the number of clusters is typically much smaller than the \n",
    "number of observations to be clustered. Each cluster is given a unique label,\n",
    "and these labels are mapped. Using the clusters' profile and label, the map of \n",
    "labels can be interpreted to get a sense of the spatial distribution of \n",
    "socio-demographic traits. The power of (geodemographic) clustering comes\n",
    "from taking statistical variation across several dimensions and compressing it\n",
    "into a single categorical one that we can visualize through a map. To\n",
    "demonstrate the variety of approaches in clustering, we will show two\n",
    "distinct but very popular clustering algorithms: k-means and Ward's hierarchical method.\n",
    "\n",
    "### K-means\n",
    "\n",
    "K-means is probably the most widely used approach to\n",
    "cluster a dataset. The algorithm groups observations into a\n",
    "pre-specified number of clusters so that that each observation is\n",
    "closer to the mean of its own cluster than it is to the mean of any other cluster.\n",
    "The k-means problem is solved by iterating between an assignment step and an update step. \n",
    "First, all observations are randomly assigned one of the $k$ labels. Next, the \n",
    "multivariate mean over all covariates is calculated for each of the clusters.\n",
    "Then, each observation is reassigned to the cluster with the closest mean. \n",
    "If the observation is already assigned to the cluster whose mean it is closest to,\n",
    "the observation remains in that cluster. This assignment-update process continues\n",
    "until no further reassignments are necessary.\n",
    "\n",
    "The nature of this algorithm requires us to select the number of clusters we \n",
    "want to create. The right number of clusters is unknown in practice. For\n",
    "illustration, we will use $k=5$ in the `KMeans` implementation from\n",
    "`scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise KMeans instance\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f13dc",
   "metadata": {},
   "source": [
    "This illustration will also be useful as virtually every algorithm in `scikit-learn`,\n",
    "the (Python) standard library for machine learning, can be run in a similar fashion.\n",
    "To proceed, we first create a `KMeans` clusterer object that contains the description of\n",
    "all the parameters the algorithm needs (in this case, only the number of clusters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ee9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise KMeans instance\n",
    "kmeans = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba92e2",
   "metadata": {},
   "source": [
    "Next, we set the seed for reproducibility and call the `fit` method to compute the algorithm specified in `kmeans` to our scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "# Run K-Means algorithm\n",
    "k5cls = kmeans.fit(db_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9bccaf",
   "metadata": {},
   "source": [
    "Now that the clusters have been assigned, we can examine the label vector, which \n",
    "records the cluster to which each observation is assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61000171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first five labels\n",
    "k5cls.labels_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5ef97",
   "metadata": {},
   "source": [
    "In this case, the first observation is assigned to cluster 2, the second and fourth ones are assigned to cluster 1, the third to number 3 and the fifth receives the label 4. It is important\n",
    "to note that the integer labels should be viewed as denoting membership only &mdash;\n",
    "the numerical differences between the values for the labels are meaningless.\n",
    "The profiles of the various clusters must be further explored by looking\n",
    "at the values of each dimension. But, before we do that, let's make a map.\n",
    "\n",
    "### Spatial Distribution of Clusters\n",
    "\n",
    "Having obtained the cluster labels, we can display the spatial\n",
    "distribution of the clusters by using the labels as the categories in a\n",
    "choropleth map. This allows us to quickly grasp any sort of spatial pattern the \n",
    "clusters might have. Since clusters represent areas with similar\n",
    "characteristics, mapping their labels allows to see to what extent similar areas tend\n",
    "to have similar locations.\n",
    "Thus, this gives us one map that incorporates the information from all nine covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels into a column\n",
    "db[\"k5cls\"] = k5cls.labels_\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot unique values choropleth including\n",
    "# a legend and with no boundary lines\n",
    "db.plot(\n",
    "    column=\"k5cls\", categorical=True, legend=True, linewidth=0, ax=ax\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-animal",
   "metadata": {},
   "source": [
    "# DBSCAN: The Tokyo photographs dataset\n",
    "\n",
    "The rise of new forms of data such as geo-tagged photos uploaded to online services is creating new ways for researchers to study and understand cities. Where do people take pictures? When are those pictures taken? Why do certain places attract many more photographers than others? All these questions and more become more than just rhetorical ones when we consider, for example,  online photo hosting services as volunteered geographic information (VGI, {cite}`Goodchild2007citizens`). In this chapter we will explore metadata from a sample of geo-referenced images uploaded to [Flickr](https://www.flickr.com/) and extracted thanks to the [100m Flickr dataset](https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67). In doing so, we will introduce a few approaches that help us better understand the distribution and characteristics of a point pattern. \n",
    "\n",
    "To get started, let's load the packages we will need in this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-jerusalem",
   "metadata": {},
   "source": [
    "Then, let us load some data about picture locations from Flickr:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"https://www.dropbox.com/s/jeiw3921imfto9q/tokyo_clean.csv?dl=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-programmer",
   "metadata": {},
   "source": [
    "The table contains the following information about the sample of 10,000 photographs: the ID of the user who took the photo; the location expressed as latitude and longitude columns; a transformed version of those coordinates expressed in Pseudo Mercator; the timestamp when the photo was taken; and the URL where the picture they refer to is stored online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-sunglasses",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "proud-honolulu",
   "metadata": {},
   "source": [
    "## Visualizing Point Patterns\n",
    "\n",
    "There are many ways to visualize geographic point patterns, and the choice of method depends on the intended message. \n",
    "\n",
    "### Showing Patterns as Dots on a Map\n",
    "\n",
    "The first step to get a sense of what the spatial dimension of this dataset looks like is to plot it. At its most basic level, we can generate a scatter plot with `sns`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-father",
   "metadata": {
    "caption": "Tokyo points jointplot, longitude and latitude.",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate scatter plot\n",
    "sns.jointplot(x=\"longitude\", y=\"latitude\", data=db, s=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-maximum",
   "metadata": {},
   "source": [
    "### Showing Density with Hex-binning\n",
    "\n",
    "Consider our second problem: cluttering. When too many photos are concentrated in some areas of, plotting opaque dots on top of one another can make it hard to discern any pattern and explore its nature. For example, in the middle of the map, towards the right, there appears to be the highest concentration of pictures taken; the sheer amount of dots on the maps in some parts obscures whether all of that area receives as many pics or whether, within there, some places receive a particularly high degree of attention.\n",
    "\n",
    "One solution to get around cluttering relates to what we referred to earlier as moving from {ref}`\"tables to surfaces\" <ch03-surfaces_as_tables>`. We can now recast this approach as a *spatial* or *2-dimensional histogram*. Here, we generate a regular grid (either squared or hexagonal), count how many dots fall within each grid cell, and present it as we would any other choropleth. This is attractive because it is simple, intuitive and, if fine enough, the regular grid removes some of the area distortions choropleth maps may induce. For this illustration, let us use use hexagonal binning (sometimes called hexbin) because it has slightly nicer properties than squared grids, such as less shape distortion and more regular connectivity between cells. Creating a hexbin 2-d histogram is straightforward in Python using the `hexbin` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-horror",
   "metadata": {
    "caption": "Tokyo points Hex Binning",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(12, 9))\n",
    "# Generate and add hexbin with 50 hexagons in each\n",
    "# dimension, no borderlines, half transparency,\n",
    "# and the reverse viridis colormap\n",
    "hb = ax.hexbin(\n",
    "    db[\"x\"],\n",
    "    db[\"y\"],\n",
    "    gridsize=50,\n",
    "    linewidths=0,\n",
    "    alpha=0.5,\n",
    "    cmap=\"viridis_r\",\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(hb)\n",
    "# Remove axes\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "timely-reality",
   "metadata": {},
   "source": [
    "## Identifying clusters\n",
    "\n",
    "From the many spatial point clustering algorithms, we will cover one called DBSCAN (Density-Based Spatial Clustering of Applications, {cite}`ester1996density`. DBSCAN is a widely used algorithm that originated in the area of knowledge discovery and machine learning and that has since spread into many areas, including the analysis of spatial points. In part, its popularity resides in its intellectual simplicity and computational tractability. In some ways, we can think of DBSCAN as a point pattern counterpart of the local statistics we explored in [Chapter 7](07_local_autocorrelation). They do however differ in fundamental ways. Unlike the local statistics we have seen earlier, DBSCAN is not based on an inferential framework, but it is instead a deterministic algorithm. This implies that, unlike the measures seen before, we will not be able to estimate a measure of the degree to which the clusters found are compatible with cases of spatial randomness.  \n",
    "\n",
    "\n",
    "\n",
    "From the point of view of **DBSCAN, a cluster is a concentration of at least `m` points, each of them within a distance of `r` of at least another point in the cluster**. Following this definition, the algorithm classifies each point in our pattern into three categories:\n",
    "\n",
    "* *Noise*, for those points outside a cluster.\n",
    "* *Cores*, for those points inside a cluster with at least `m` points in the cluster within distance `r`.\n",
    "* *Borders* for points inside a cluster with less than `m` other points in the cluster within distance `r`.\n",
    "\n",
    "Here is a cluster where `m` is 3 points for a given `r` distance.\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/400px-DBSCAN-Illustration.svg.png\" alt=\"drawing\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "The flexibility (but also some of the limitations) of the algorithm resides in that both `m` and `r` need to be specified by the user before running DBSCAN. This is a critical point, as their value can influence significantly the final result. Before exploring this in greater depth, let us get a first run at computing `DBSCAN` in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DBSCAN\n",
    "clusterer = DBSCAN()\n",
    "# Fit to our data\n",
    "clusterer.fit(db[[\"x\", \"y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-snowboard",
   "metadata": {},
   "source": [
    "Following the standard interface in scikit-learn, we first define the algorithm we want to run (creating the `clusterer` object) and then we *fit* it to our data. Once fit, `clusterer` contains the required information to access all the results of the algorithm. The `core_sample_indices_` attribute contains the indices (order, starting from zero) of each point which is classified as a *core*. We can have a peek into it to see what it looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-principle",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Print the first 5 elements of `cs`\n",
    "clusterer.core_sample_indices_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-budget",
   "metadata": {},
   "source": [
    "The printout above tells us that the second (remember, Python starts counting at zero!) point in the dataset is a core, as are the 23rd, 31st, 36th, and 43rd points. This attribute has a variable length, depending on how many cores the algorithm finds.\n",
    "\n",
    "The second attribute of interest is `labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-insulin",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "clusterer.labels_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-smart",
   "metadata": {},
   "source": [
    "The labels object always has the same length as the number of points used to run DBSCAN. Each value represents the index of the cluster a point belongs to. If the point is classified as *noise*, it receives a -1. Above, we can see that the second point belongs to cluster 1, while the others in the list are effectively not part of any cluster. To make thinks easier later on, let us turn the labels into a `Series` object that we can index in the same way as our collection of points:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = pd.Series(clusterer.labels_, index=db.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-reviewer",
   "metadata": {},
   "source": [
    "Now we already have the clusters, we can proceed to visualize them. There are many ways in which this can be done. We will start just by coloring points in a cluster in red and noise in gray:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-fever",
   "metadata": {
    "caption": "Tokyo points, DBSCAN clusters.",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Subset points that are not part of any cluster (noise)\n",
    "noise = db.loc[lbls == -1, [\"x\", \"y\"]]\n",
    "# Plot noise in grey\n",
    "ax.scatter(noise[\"x\"], noise[\"y\"], c=\"grey\", s=5, linewidth=0)\n",
    "# Plot all points that are not noise in red\n",
    "# NOTE how this is done through some fancy indexing, where\n",
    "#      we take the index of all points (tw) and substract from\n",
    "#      it the index of those that are noise\n",
    "ax.scatter(\n",
    "    db.loc[db.index.difference(noise.index), \"x\"],\n",
    "    db.loc[db.index.difference(noise.index), \"y\"],\n",
    "    c=\"red\",\n",
    "    linewidth=0,\n",
    ")\n",
    "\n",
    "# Remove axes\n",
    "ax.set_axis_off()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-applicant",
   "metadata": {},
   "source": [
    "Although informative, the result of this run is not particularly satisfactory. There are *way* too many points that are classified as \"noise\".\n",
    "\n",
    "This is because we have run DBSCAN with the default parameters: a radius of 0.5 and a minimum of five points per cluster. Since our data is expressed in meters, a radius of half a meter will only pick up hyper local clusters. This might be of interest in some cases but, in others, it can result in odd outputs. \n",
    "\n",
    "If we change those parameters, can pick up more general patterns. For example, let us say a cluster needs to, at least, have roughly 1% of all the points in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-mentor",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Obtain the number of points 1% of the total represents\n",
    "minp = numpy.round(db.shape[0] * 0.01)\n",
    "minp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-belle",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "At the same time, let us expand the maximum radius to say, 500 meters. Then we can re-run the algorithm and plot the output, all in the same cell this time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-december",
   "metadata": {
    "caption": "Tokyo points, Clusters with DBSCAN and minp=0.01.",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "# Rerun DBSCAN\n",
    "clusterer = DBSCAN(eps=500, min_samples=int(minp))\n",
    "clusterer.fit(db[[\"x\", \"y\"]])\n",
    "# Turn labels into a Series\n",
    "lbls = pd.Series(clusterer.labels_, index=db.index)\n",
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Subset points that are not part of any cluster (noise)\n",
    "noise = db.loc[lbls == -1, [\"x\", \"y\"]]\n",
    "# Plot noise in grey\n",
    "ax.scatter(noise[\"x\"], noise[\"y\"], c=\"grey\", s=5, linewidth=0)\n",
    "# Plot all points that are not noise in red\n",
    "# NOTE how this is done through some fancy indexing, where\n",
    "#      we take the index of all points (db) and substract from\n",
    "#      it the index of those that are noise\n",
    "ax.scatter(\n",
    "    db.loc[db.index.difference(noise.index), \"x\"],\n",
    "    db.loc[db.index.difference(noise.index), \"y\"],\n",
    "    c=\"red\",\n",
    "    linewidth=0,\n",
    ")\n",
    "\n",
    "# Remove axes\n",
    "ax.set_axis_off()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b8423dd",
   "metadata": {},
   "source": [
    "## Q.2\n",
    "Suggest a scenario in your own work or research when you might want to use a DBSCAN algorithm with a larger `r` and and `m`. Suggest a scenario where you may want to use smaller parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f6705",
   "metadata": {},
   "source": [
    "INSERT YOUR TEXT HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc4627",
   "metadata": {},
   "source": [
    "## Q.3\n",
    "In the San Diego k-means clustering example, try running the clustering with K=10, and K=20. What are the results that you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db980022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
