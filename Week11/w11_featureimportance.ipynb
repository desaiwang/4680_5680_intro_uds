{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-gambling",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99a7a244",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Use cross-validation\n",
    "- Find the best model for classification and regression problems based on tuning hyperparameters and calculating performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-introduction",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "from pysal.lib import weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17628626",
   "metadata": {},
   "source": [
    "## 0.1 Import the data\n",
    "\n",
    "For this exercise, let's use the San Diego AirBnB data set again. As a reminder: This dataset contains house intrinsic characteristics, both continuous (number of beds as in `beds`) and categorical (type of renting or, in Airbnb jargon, property group as in the series of `pg_X` binary variables), but also variables that explicitly refer to the location and spatial configuration of the dataset (e.g., distance to Balboa Park, `d2balboa` or neighborhood id, `neighborhood_cleansed`).\n",
    "\n",
    "\n",
    "Our aim here is to predict the **`log_price`** (Regression) of an AirBnB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b832b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = gpd.read_file(\"https://www.dropbox.com/s/zkucu7jf1xug869/regression_db.geojson?dl=1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ed271",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5182eb",
   "metadata": {},
   "source": [
    "Again, notice here that we have: \n",
    "- **Discrete variables** (number of bedrooms, beds, baths)\n",
    "- **Dummy variables** (whether there is a pool, whether near the coast, room type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a770dfc6",
   "metadata": {},
   "source": [
    "# 1. `log_price`\n",
    "Let's start off with predicting the price of the Airbnb. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b565af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = db['log_price']\n",
    "\n",
    "## We'll make our X, independent variables, the \"kitchen sink\" of all of our other variables for now. \n",
    "## I'm using all the variables we have available with the exception of `neighborhood`, which we have to turn into dummy variables in a second. \n",
    "\n",
    "X = db[['accommodates', 'bathrooms', 'bedrooms', 'beds', 'pool',\n",
    "       'd2balboa', 'coastal', 'pg_Apartment',\n",
    "       'pg_Condominium', 'pg_House', 'pg_Other', 'pg_Townhouse',\n",
    "       'rt_Entire_home/apt', 'rt_Private_room', 'rt_Shared_room']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2cba824",
   "metadata": {},
   "source": [
    "## 1.1 Feature engineering\n",
    "Feature engineering is the process of creating new variables from the ones you already have. Common feature engineering tasks include:\n",
    "- Creating dummy variables from categorical variables\n",
    "- Creating interaction terms between variables\n",
    "- Creating polynomial terms from variables\n",
    "- Creating log or square root terms from variables\n",
    "- Creating lagged variables from time series data or lagged spatial variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dummies = pd.get_dummies(db['neighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f31ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce981b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, I want to concatenate my X and neighborhood_dummies into one dataframe.\n",
    "## I need to tell pd.concat() to either add new columns (axis=1) or add new rows (axis=0).\n",
    "## The default is axis=0, ie new rows, so I need to specify axis=1.\n",
    "X = pd.concat([X, neighborhood_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d711091",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8da70da6",
   "metadata": {},
   "source": [
    "Let's also create a new column that is the KNN spatial lag for the 'neighborhood context'. Here, I'm going to use the columns: \n",
    "- `pool`, which is a binary (0,1) variable for whether the airbnb has a pool\n",
    "- `pg_House` which is a binary (0,1) variable for whether the airbnb is a house\n",
    "\n",
    "I will choose K=20, to give me the 20 closest neighboring Airbnbs. My spatial lag should be a number between 0 and 20 to estimate, of the 20 closest Airbnb, how many have pools and how many are other housees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = weights.KNN.from_dataframe(db, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_lag = weights.lag_spatial(knn, db['pool'])\n",
    "house_lag = weights.lag_spatial(knn, db['pg_House'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6857883b",
   "metadata": {},
   "source": [
    "Add these to new features to our original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['pool_lag'] = pool_lag\n",
    "X['house_lag'] = house_lag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f668afa",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Create our Train-Test Split\n",
    "We almost always start off with splitting our data into our **train** and **test** sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Let's use the default split for now, which is 75-25 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e90bb038",
   "metadata": {},
   "source": [
    "## 1.2 Predict the data\n",
    "Here, let's use a decision tree regressor to predict the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7273cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## Use the permutation importance function from sklearn.inspection\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ee508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c24418e0",
   "metadata": {},
   "source": [
    "## 1.3 Cross-validation\n",
    "Now we use the k-fold cross-validation method is run our model several times on different parts of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c798a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The default scoring metric for Random Forest is R^2, so we can use cross_val_score() to get the R^2 for each fold.\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1bae9e7",
   "metadata": {},
   "source": [
    "As you can see, there is some variation here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17522117",
   "metadata": {},
   "source": [
    "## 1.4 Test score \n",
    "Let's see how well our model does on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the second set of data\n",
    "ypred_rf = model.predict(X_test)\n",
    "print(\"R^2 is:\", r2_score(y_test, ypred_rf))\n",
    "print(\"Mean absolute error is:\", mean_absolute_error(y_test, ypred_rf))\n",
    "print(\"Mean squared error is:\", mean_squared_error(y_test, ypred_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82c20577",
   "metadata": {},
   "source": [
    "## 1.5 Feature Importance for the Test Set\n",
    "Note: You can also estimate the importance for the training set, but generally we're interested in performance for new data (test set). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c64c47",
   "metadata": {},
   "source": [
    "### 1.5.1 Impurity based feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd131ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "std = np.std([model.feature_importances_ for tree in model.estimators_], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31551b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,6))\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "166e95e5",
   "metadata": {},
   "source": [
    "### 1.5.2 Permutation based feature importance\n",
    "This will take a few seconds because we have to reshuffle each column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "forest_importances = pd.Series(result.importances_mean, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,6))\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65b13aad",
   "metadata": {},
   "source": [
    "## Q.1 Feature Importances \n",
    "Tune the hyperparameters on your model and measure the feature importances using the impurity and permutation based feature importances. Did your importances change?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
