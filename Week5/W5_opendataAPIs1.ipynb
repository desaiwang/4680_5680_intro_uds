{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "homeless-heart",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Get Census data from the U.S. Census API\n",
    "- Use the Socrata API\n",
    "\n",
    "\n",
    "Some of today's lessons borrow from: \n",
    "- [PyGIS - Open Source Spatial Programming and Remote Sensing book](https://pygis.io/docs/d_access_census.html)\n",
    "- [The Socrata SODA API documentation](https://dev.socrata.com/consumers/getting-started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766368de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You might need to run these or manually add the libraries to your environment in Anaconda\n",
    "# !pip install census\n",
    "# !pip install us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to start importing the libraries we need\n",
    "# all in one cell. \n",
    "# It is a good practice to keep all the imports in one cell so that\n",
    "# we can easily see what libraries we are using in the notebook.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## The set_context() function is really useful!\n",
    "## It allows us to set the size of the fonts in our plots based on whether \n",
    "## we are making a poster, a talk, a notebook, etc.\n",
    "\n",
    "## If you are only presenting these figures in your jupyter notebook, \n",
    "## there is no need to set the context to be \"talk\" or \"poster\"\n",
    "## But, I sometimes set my context to be \"talk\" or \"poster\" even for articles\n",
    "## because I like the fonts to be bigger.\n",
    "sns.set_context(context='paper')\n",
    "\n",
    "# we use the inline backend to generate the plots within the browser\n",
    "%matplotlib inline\n",
    "\n",
    "from census import Census\n",
    "from us import states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd8b51",
   "metadata": {},
   "source": [
    "\n",
    "# 0. Census Data: Census survey and statistical boundaries\n",
    "\n",
    "## 0.1 Census Surveys\n",
    "The United States Census Bureau has been collecting information on its residents in the country since 1780 through surveys sent by mail (since 2020, you can submit your survey by phone, mail, or online). Census data is used for a variety of governmental purposes including: provision of housing, infrastructure, and public amenities; making districting decisions for schools, precints, and elections; and more generally, to understand the population, socio-economic, and demographic characteristics of residents in the country. [Did you know that the punch card machine (a prototype for the computer) was created for the 1890 Census?](https://en.wikipedia.org/wiki/Tabulating_machine)\n",
    "\n",
    "The US Census has historically been taken every 10 years. Every household in the U.S. is sent a Census survey (and you are legally required to respond.) In 2005, the Census Bureau created the American Community Survey (ACS), which is collected every month on a sample of households.\n",
    "\n",
    "Since 2020, the Census only contains 10 questions (historically called the \"short form census\") such as age, sex, race, Hispanic origin, and owner/renter status. The ACS contains a larger set of questions such as employment, education, transportation.\n",
    "\n",
    "Because the ACS is more frequent, it is often used for more current census needs; however, because it is also a sample, we generally need a longer time span to get a robust sample. This is why we will often use the **5-yr ACS** (for ex: 2012 - 2016 ACS) to represent the year (here, 2014).\n",
    "\n",
    "Census data is often the baseline survey dataset in the area of urban planning because it provides racial, socio-economic, housing, etc. information that is often the highlight or backdrop of a study.\n",
    "\n",
    "## 0.2 Census Geographies\n",
    "There are different, often nested Census geographic regions used for  different administrative scales. The most commonly used regions are statistical areas, typically nested within each other, whose boundaries are defined by certain physical, administrative, and population constraints. For instance, a **Census block** is bounded by physical features such as streets and administrative boundaries such as city limits and school districts. **Block groups**, the smallest unit of analysis that is still mostly statistically robust, are collections of Census blocks (hence the name) that generally have between 800 to 5000 people. **Census tracts** generally have between 1000 and 8000 people. [Here's more information](https://pitt.libguides.com/uscensus/understandinggeography) about Census geographies if you're curious.\n",
    "\n",
    "See the image below for how these regions nest within one another.\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/8w69pibhwffgoc0/qgis_censusgeography.png?dl=1\" alt=\"drawing\" width=\"500\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "## 0.3 [Social Explorer](https://www-socialexplorer-com.proxy.library.cornell.edu/ezproxy)\n",
    "This is a great tool for looking at Census and ACS data visually. They also have datasets beyond just Census Bureau data. You can also output images and shareable links to the map. I encourage you to sign up (through Cornell it's free) and explore this tool on your own time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff145b",
   "metadata": {},
   "source": [
    "# 0.4 What is an API \n",
    "APIs are tools that allow different software applications to communicate with one another. In particular, the Census API allows us to access data from the US Census Bureau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee71fa",
   "metadata": {},
   "source": [
    "# 1. U.S. Census \n",
    "The Census makes data publicly available directly from their website `census.gov`. They have a bunch of APIs on their website that allow you to access various datasets: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/scl/fi/glfgqu8evzpqql70eq08w/Screen-Shot-2024-02-11-at-11.27.10-AM.png?rlkey=e6gio2bjp7w1eadjf2v3tecp2&dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757c451",
   "metadata": {},
   "source": [
    "## 1.1 Census Python Package\n",
    "The `census` python library is a wrapper for the US Census API. We are also going to use a helper tool called `us` that helps us to navigate the FIPS codes and other US State metadata like capitals, time zones, postal codes, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8365e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.VA.fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.NY.fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966110e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.NY.capital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33347ba1",
   "metadata": {},
   "source": [
    "You will need to create and keep track of your Census API key, which can be obtained [here](http://api.census.gov/data/key_signup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key\n",
    "c = Census(\"YOUR CENSUS API KEY HERE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aed797",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(\"d9c002dc1334c8f6cbea48d3f10a4176cdf89064\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a24cd0",
   "metadata": {},
   "source": [
    "## 1.2 Getting the ACS 5-year\n",
    "There are various geographies at which we can get the ACS 5-year tables, here are the functions and inputs: \n",
    "\n",
    "* state(fields, state_fips)\n",
    "* state_county(fields, state_fips, county_fips)\n",
    "* state_county_blockgroup(fields, state_fips, county_fips, blockgroup)\n",
    "* state_county_subdivision(fields, state_fips, county_fips, subdiv_fips)\n",
    "* state_county_tract(fields, state_fips, county_fips, tract)\n",
    "* state_place(fields, state_fips, place)\n",
    "* state_congressional_district(fields, state_fips, congressional_district)\n",
    "* state_legislative_district_upper(fields, state_fips, legislative_district)\n",
    "* state_legislative_district_lower(fields, state_fips, legislative_district)\n",
    "* us(fields)\n",
    "* state_zipcode(fields, state_fips, zip5)\n",
    "\n",
    "You can consult the [documentation](https://pypi.org/project/census/) to see which vintages the library has. It looks like they only have up to the 2021 5YR (2017-2021). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b2204",
   "metadata": {},
   "source": [
    "Going on the [ACS 5Yr page on the census website](https://www.census.gov/data/developers/data-sets/acs-5year.html) (make sure to select the correct year!), we can see the different types of tables that exist. \n",
    "\n",
    "We are interested in columns from the \"[Detailed Tables](https://api.census.gov/data/2021/acs/acs5/variables.html)\" here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7b8b8",
   "metadata": {},
   "source": [
    "We can also use the [Table Shells and Table List](https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.2021.html#list-tab-79594641) to more quickly look for the columns we need. \"The ACS table list contains columns with the table IDs, table titles, table universes, and 1-year/5-year availability for all Detailed Tables, Supplemental Estimate Tables, Comparison Profiles, Data Profiles, and Subject Tables in one spreadsheet.\"\n",
    "\n",
    "You will have to download the `XXXX ACS Detailed Table Shells` for the ACS 1/5 YR if you want to use the table shells. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/scl/fi/gx1q7o27byz9lt83o6ekr/Screen-Shot-2024-02-12-at-11.17.28-AM.png?rlkey=s4y1r0gathbpnu0vqs4z4zrtf&dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2891e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B16010_041E: is the total number of people with an educational attainment of a bachelor's degree or higher\n",
    "# B01003_001E: total population\n",
    "ny_census = c.acs5.state_county_tract(fields = ('NAME', 'B16010_041E','B01003_001E'),\n",
    "                                      state_fips = states.NY.fips,\n",
    "                                      county_fips = \"*\",\n",
    "                                      tract = \"*\",\n",
    "                                      year = 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa20626",
   "metadata": {},
   "source": [
    "We do need to create a `GEOID` column that's the actual FIPS code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe473490",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df = pd.DataFrame(ny_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df[\"GEOID\"] = ny_df[\"state\"] + ny_df[\"county\"] + ny_df[\"tract\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeeec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e40164",
   "metadata": {},
   "source": [
    "We can also translate the number of people with a college degree or higher to a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e621265",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df['college_ed_perc'] = ny_df['B16010_041E'] / ny_df['B01003_001E'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5daa4c",
   "metadata": {},
   "source": [
    "## 1.3 Get the shapefiles \n",
    "The Census also maintains a [set of shapefiles](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html) that has the geometries by state, counties, tracts, block groups, and more. \n",
    "\n",
    "When you go to the Tiger/Line Shapefiles, make sure to select the year you are looking for: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/scl/fi/qbbj4x6jer4sjtldb228u/Screen-Shot-2024-02-12-at-10.29.16-AM.png?rlkey=i6t2k6zr8e83ofy4rjh58pou8&dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "You can use the **FTP Archive** to find the particular boundary and state you need (you'll have to know the FIPS code for the state):\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/scl/fi/wj3ewuazhzx84c5bbtnjm/Screen-Shot-2024-02-12-at-10.40.24-AM.png?rlkey=zxresvebgfods8p69yjqolde4&dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8911d2",
   "metadata": {},
   "source": [
    "Once you have all this information, you can read the shapefile directly from the URL link: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67487dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_tract = gpd.read_file(\"https://www2.census.gov/geo/tiger/TIGER2019/TRACT/tl_2019_36_tract.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31007be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_tract.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_tract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee50bba",
   "metadata": {},
   "source": [
    "Finally, we can merge the tables we created with the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_census_geo = ny_tract.merge(ny_df, left_on = 'GEOID', right_on = 'GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_census_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5aa64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_census_geo.plot('college_ed_perc', legend = True, figsize = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328af4e7",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "List at least one reason why the above is not a clear figure in a markdown cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af15b3a",
   "metadata": {},
   "source": [
    "INSERT YOUR TEXT HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a18968",
   "metadata": {},
   "source": [
    "## Q.2 \n",
    "Using the [ACS table](https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.2019.html#list-tab-LO1F1MU1CQP3YOHD2T) lookup page to download \"2019 ACS Detailed Table Shells\"\n",
    "- Find the table ID (in the format of \"B00000\") for the **HISPANIC OR LATINO ORIGIN BY RACE** table \n",
    "- Plot the percentage Hispanic or Latino Origin by Race for Oregon using the method we described above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "or_census_geo = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ec21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "or_census_geo.plot(### INSERT YOUR CODE HERE)\n",
    "ax.set_axis_off()\n",
    "\n",
    "## Use tight_layout to remove the white space around the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "## I forgot to show you all how to save down your plots!\n",
    "fig.savefig('OR_perc_hispanic.png')   # save the figure to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a6c51",
   "metadata": {},
   "source": [
    "## 2. Socrata and Socrata APIs\n",
    "Many government open data portals were built by the same company, Socrata (acquired a few years back by Tyler Technologies), which created the infrastructure and front-end interface to access open government data. \n",
    "\n",
    "We are going to look at Mandatory Inclusionary Housing zones in New York City [here](https://data.cityofnewyork.us/Housing-Development/Mandatory-Inclusionary-Housing-MIH-/bw8v-wzdr).\n",
    "\n",
    "\n",
    "You may have noticed that, when we go to export data, that there is a **SODA API** section: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/0ewtgsg8lc4sl3j/Screen%20Shot%202024-02-12%20at%2011.30.04%20AM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "SODA is Socrata's API for allowing users from researchers to (more often) people building tools and applications to access open-portal data. This is most useful when you have to programmatically connect your data export to something else. For instance, if you're running a website that needs to update data in real-time or if you don't want to download an updated dataset each time, you can connect your notebook or app to this API. Click to expand the **SODA API** section.\n",
    "\n",
    "\n",
    "**Copy the API endpoint URL**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01f24f",
   "metadata": {},
   "source": [
    "## 2.1 API endpoint to GeoDataFrame\n",
    "\n",
    "We can pretty easily this JSON file into a geodataframe. FYI, a JSON stands for \"JavaScript Object Notation\" and is a file format that was desisgned for the JavaScript language, but is easily translated to other formats that we know well. \n",
    "\n",
    "The good thing is that pandas has a `pd.read_json()` function that will allow us read this JSON as a DF and eventually turn it into a geodataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mih = pd.read_json('https://data.cityofnewyork.us/resource/m79g-k9r4.json')\n",
    "mih = pd.read_json('INSERT_YOUR_API_ENDPOINT_HERE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99913fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ab6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63284f5",
   "metadata": {},
   "source": [
    "Notice that there is a **the_geom** column that looks like it might have geometry information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ignore the warnings \n",
    "mih['the_geom'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75987309",
   "metadata": {},
   "source": [
    "We are going to turn these strings, into Shapely geometries, which is the only piece of our data that is missing so we can turn this into a geometry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e487e24-f6f6-46a3-8c78-c379e09394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1de13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "\n",
    "## the apply method applies the function to each row of the dataframe\n",
    "mih['the_geom'] = mih['the_geom'].apply(shape)\n",
    "\n",
    "## I'm going to use the GeoDataFrame method to create a GeoDataFrame\n",
    "## I figured the CRS is 4326 looking at the lat/longs in the geometries, but unfortunately \n",
    "## we are not given the CRS in the data documentation!\n",
    "## We can look at the shapefile .prj file to see what the CRS is.\n",
    "mih_geo = gpd.GeoDataFrame(mih,geometry='the_geom',crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faint, but these are our buildings\n",
    "\n",
    "mih_geo.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeb266",
   "metadata": {},
   "source": [
    "## 2.2 Filtering\n",
    "The SODA API allows us to filter data from the endpoint url. Why might we want to do this? For one, there are very large datasets such as the [311 Service Requests dataset](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) (with 32 million rows) or the [Open Parking and Camera Violations](https://data.cityofnewyork.us/City-Government/Open-Parking-and-Camera-Violations/nc67-uf89) (with 93 million rows!) that are difficult to work with due to their size. \n",
    "\n",
    "There are two ways to filter data using the SODA API: \n",
    "- [Simple Filters](https://dev.socrata.com/docs/filtering.html)\n",
    "- [SoQL Queries](https://dev.socrata.com/docs/queries/)\n",
    "\n",
    "\n",
    "**Both of these filters are text we append to the original endpoint URL.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996410ef",
   "metadata": {},
   "source": [
    "### 2.2.1 Simple Filters\n",
    "Any column in the dataset can be used as a filter for specific values within that column and is in the format :\n",
    "\n",
    "`http://yourendpointurl.json?col_name=element_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_url_orig = \"https://data.cityofnewyork.us/resource/m79g-k9r4.json\"\n",
    "\n",
    "## Note, this query is CASE-SENSITIVE! \n",
    "## If the column name is in all caps, it must be in all caps here\n",
    "## If the value of interest is in all caps, it must be in all caps here\n",
    "mih_url_mh = \"https://data.cityofnewyork.us/resource/m79g-k9r4.json?Boro=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f394ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_mh = pd.read_json(mih_url_mh)\n",
    "mih_mh['the_geom'] = mih_mh['the_geom'].apply(shape)\n",
    "mih_mh_geo = gpd.GeoDataFrame(mih_mh,geometry='the_geom',crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_mh_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78991888",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_mh_geo.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383cb9",
   "metadata": {},
   "source": [
    "You can join multiple queries with an `&`.\n",
    "\n",
    "**One key formatting difference here is the use of white space, but must be translated into `%20` for URL purposes, since no white spaces are allowed in the URL.** I am using the `.replace(\"to_be_replace_str\",\"new_str\")` function to replace empty spaces with `%20`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_url_mh_eh = \"https://data.cityofnewyork.us/resource/m79g-k9r4.json?boro=1&project_nam=East Harlem Neighborhood Rezoning\".replace(' ','%20')\n",
    "nycha_url_mh_eh = pd.read_json(nycha_url_mh_eh)\n",
    "nycha_url_mh_eh['the_geom'] = nycha_url_mh_eh['the_geom'].apply(shape)\n",
    "nycha_url_mh_eh_geo = gpd.GeoDataFrame(nycha_url_mh_eh,geometry='the_geom',crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84446d-d28b-41e9-9f6e-d003f229fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_url_mh_eh_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_url_mh_eh_geo.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006aade",
   "metadata": {},
   "source": [
    "### 2.2.2 SoQL Queries\n",
    "The “Socrata Query Language” (SoQL) is a simple, SQL-like query language specifically designed for making it easy to work with data on the web. If you're familiar with SQL, the following may be familiar. And even if you're not, this will seem pretty intuitive. \n",
    "\n",
    "Here are all the different parameters that you can use in this query: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/r4edgdtyzm2vrxn/Screen%20Shot%202023-02-19%20at%2010.09.27%20AM.png?dl=1\" alt=\"drawing\" width=\"800\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "The same filtering for Manhattan and the Jefferson Development we did above would look like this: \n",
    "\n",
    "(Note that the values we need to filter by need single quotes if they are strings now.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ea5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note the use of single vs double quotes here, since I need to include a single quote in the query\n",
    "nycha_url_mh_eh_sql = \"https://data.cityofnewyork.us/resource/m79g-k9r4.json?$where=boro='1' and project_nam='East Harlem Neighborhood Rezoning'\".replace(\" \", \"%20\")\n",
    "\n",
    "nycha_url_mh_eh2 = pd.read_json(nycha_url_mh_eh_sql)\n",
    "nycha_url_mh_eh2['the_geom'] = nycha_url_mh_eh2['the_geom'].apply(shape)\n",
    "nycha_url_mh_eh2_geo = gpd.GeoDataFrame(nycha_url_mh_eh2,geometry='the_geom',crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_url_mh_eh2_geo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075bb44",
   "metadata": {},
   "source": [
    "### 2.2.3 A more complex SoQL query\n",
    "\n",
    "Let's say we wanted to look at the [311 Service Requests data](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9). Here are the ways I want to filter the dataset based on the columns available: \n",
    "- **Created Date** is since Feb 2023\n",
    "- **Complaint Type**  is `Noise - Residential`\n",
    "- **Descriptor** is `Loud Music/Party` \n",
    "\n",
    "Looking at the [311 API docs](https://dev.socrata.com/foundry/data.cityofnewyork.us/erm2-nwe9) will give you some example queries and will also show you the correct column names for the API. You can also find the column names when you click on each column in the \"Columns in the Dataset\" section of the data homepage. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/wlrh8jzes9dcsvv/Screen%20Shot%202023-02-19%20at%2011.55.08%20AM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_url = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-21T0:00:00.000'   and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\")\n",
    "servicereq = pd.read_json(servicereq_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ec74a-3539-4e0d-9b87-a868d2f02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe6ea0",
   "metadata": {},
   "source": [
    "Let's turn this into a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_geo[servicereq_geo.geometry.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_geo = gpd.GeoDataFrame(servicereq, \n",
    "                                  geometry=gpd.points_from_xy(servicereq['longitude'], \n",
    "                                                              servicereq['latitude']),\n",
    "                                                              crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee374ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_geo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07289578",
   "metadata": {},
   "source": [
    "## 2.3 `offset` and `limit`\n",
    "The issue with using this endpoint is that we are limited to 1000 rows per query. You will see the documentation refer to this as \"pages\" sometimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afcf16",
   "metadata": {},
   "source": [
    "What to do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda8430",
   "metadata": {},
   "source": [
    "One way to get around this is to use the `limit` and `offset` parameters. From the SODA documentation: \n",
    "\n",
    ">The `$offset` parameter is most often used in conjunction with $limit to page through a dataset. The `$offset` is the number of records into a dataset that you want to start, indexed at 0. For example, to retrieve the “4th page” of records (records 151 - 200) where you are using `$limit` to page 50 records at a time, you’d ask for an `$offset` of 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset=150&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-21T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\")\n",
    "servicereq_offset = pd.read_json(servicereq_url_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4b196",
   "metadata": {},
   "source": [
    "This is now 50 entries of the \"4th page\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4af51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a606b",
   "metadata": {},
   "source": [
    "So, to get all the data, what we can do is run a loop to change that offset amount iteratively. \n",
    "\n",
    "OR\n",
    "\n",
    "If we are getting the data just once, we can use the filter function, accessible through the  \"View Data\" button on the dataset's home page. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/oz26ti7y164pm8r/Screen%20Shot%202023-02-19%20at%2012.35.21%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73a535",
   "metadata": {},
   "source": [
    "### 2.3.1 A short review of loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counter = np.arange(0,1000,50)\n",
    "print(my_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14336041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The for loop will iterate through each value in the list\n",
    "# The {} is a placeholder for the value in the list within a string\n",
    "for i in my_counter:\n",
    "    print(\"Current Counter is now at {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset i to 0\n",
    "i = 0\n",
    "## The while loop will continue to run until the condition is no longer true\n",
    "while i < 1000:\n",
    "    print(\"Current Counter is now at {}\".format(i))\n",
    "    \n",
    "    ## This is an example of an incrementer\n",
    "    ## An incrementer is a variable that is used to increment a value\n",
    "    ## After each iteration, the value of i will increase by 50\n",
    "    i = i + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be77f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0,100000,50):\n",
    "    print(\"Current Counter is now at {}\".format(i))\n",
    "    i = i + 50\n",
    "\n",
    "    if i >1000:\n",
    "        print(\"We are done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5496d6a",
   "metadata": {},
   "source": [
    "To programmatically run different queries, I just going to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d7f52",
   "metadata": {},
   "source": [
    "This might take a while to run and might not work at all given our 1000 an hour limit. :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb66f3-377f-4349-b465-93a56f33daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_list_smaller = np.arange(0,200,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc65168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I actually don't know what the upper range is for my dataset, but I will just use 100,000\n",
    "# offset_list = np.arange(0,100000,50)\n",
    "\n",
    "# I'm actually going to use a smaller list for demo and not overloading the API\n",
    "offset_list_smaller = np.arange(0,200,50)\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "for offset in offset_list_smaller:\n",
    "    servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset={}&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\").format(offset)\n",
    "    servicereq_offset = pd.read_json(servicereq_url_offset)\n",
    "\n",
    "    ## Here I am creating a list of dataframes by appending each dataframe to the list\n",
    "    list_of_dfs.append(servicereq_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34713251",
   "metadata": {},
   "source": [
    "I now have a list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0776095",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pd.concat will concatenate the dataframes in the list\n",
    "## to create a single dataframe\n",
    "servicereq_final = pd.concat(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c12274-97f4-4161-9cae-c85952115ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7dbb4",
   "metadata": {},
   "source": [
    "If I were to really try and get all this data, I'd put a `sleep()` call from the library `time` to pause my code from running the next line for a certain amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "for offset in offset_list_smaller:\n",
    "    servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset={}&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\").format(offset)\n",
    "    servicereq_offset = pd.read_json(servicereq_url_offset)\n",
    "\n",
    "    ## Here I am creating a list of dataframes by appending each dataframe to the list\n",
    "    list_of_dfs.append(servicereq_offset)\n",
    "    \n",
    "    ## I am adding a sleep timer to avoid overloading the API\n",
    "    ## The sleep timer will pause the code for 10 seconds\n",
    "    ## This gives me 10 seconds /run for each 50 records\n",
    "    time.sleep(10)\n",
    "    if servicereq_offset.shape[0] == 0:\n",
    "        print(\"We are done\")\n",
    "        break\n",
    "\n",
    "servicereq_final = pd.concat(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304b44d",
   "metadata": {},
   "source": [
    "Lastly! Don't think this means we can just get all the data at once. Each query we make \"costs\" the API provider resources. To ensure that everyone is able to use the API, the provider will limit your capacity to query. Here's their language on it:\n",
    "\n",
    ">## Throttling and Application Tokens\n",
    ">Hold on a second! Before you go storming off to make the next great open data app, you should understand how SODA handles throttling. You can make a certain number of requests without an application token, but they come from a shared pool and you’re eventually going to get cut off.\n",
    ">\n",
    ">If you want more requests, sign up for a Socrata account, then register for an application token and your application will be granted up to 1000 requests per rolling hour period. If you need even more than that, special exceptions are made by request. You can contact our support team here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846f09",
   "metadata": {},
   "source": [
    "## Q.3 Querying and Concatenating \n",
    "- Using the [Film Permits](https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p) dataset to retrieve two dataframes: \n",
    "    1. The **StartDateTime** should be after July 1, 2022\n",
    "    2. The **StartDateTime** should be after July 1, 2022 & The **Category** should be `Television`. \n",
    "- Create a list of two dataframes with 50 rows per \"page\"\n",
    "- Concatenate these two dataframes together into one dataframe\n",
    "- Show the first 5 rows of the new dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81311c",
   "metadata": {},
   "source": [
    "Using the [Film Permits](https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p) dataset to retrieve two dataframes: \n",
    "1. The **StartDateTime** should be after July 1, 2022\n",
    "2. The **StartDateTime** should be after July 1, 2022 & The **Category** should be `Television`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_url1 = ## INSERT YOUR CODE HERE\n",
    "film1 = pd.read_json(film_url1)\n",
    "\n",
    "film_url2 = ## INSERT YOUR CODE HERE\n",
    "film2 = pd.read_json(film_url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fa907",
   "metadata": {},
   "source": [
    "Concatenate these two dataframes together into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064c12b",
   "metadata": {},
   "source": [
    "Show the first 5 rows of the new dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5b5e944a9b71ce46e873efe99abd044b4b2cd2a5153fea1f2fc73581012ba31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
